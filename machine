import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
plt.rcParams['font.sans-serif']=['SimHei']
plt.rcParams['axes.unicode_minus']=False
if __name__ == '__main__':
 train= pd.read_csv(r'C:\Users\Administrator\Desktop\train.csv')
test= pd.read_csv(r'C:\Users\Administrator\Desktop\test.csv')
full = pd.concat([train, test], ignore_index=True)
# 显示所有列
# pd.set_option('display.max_columns', None)
# pd.set_option('display.max_columns', 5)  # 最多显示五列
# 显示所有行
# pd.set_option('display.max_rows', None)
print(full.head())  # 默认显示5行
print(full.isnull().sum()) # 查看数据的缺失情况
print(full.Embarked.mode())
full['Embarked'].fillna('S',inplace=True)
full[full.Fare.isnull()]
full.Fare.fillna(full[full.Pclass==3]['Fare'].median(),inplace=True)
full.loc[full.Cabin.notnull(),'Cabin']=1
full.loc[full.Cabin.isnull(),'Cabin']=0
full.Cabin.isnull().sum()
pd.pivot_table(full,index=['Cabin'],values=['Survived']).plot.bar(figsize=(8,5))
plt.title('存活')
plt.show()

cabin=pd.crosstab(full.Cabin,full.Survived)
cabin.rename(index={0:'无客舱号',1:'有客舱号'},columns={0.0:'死亡',1.0:'存活'},inplace=True)
cabin.plot.bar(figsize=(8,5))
plt.xticks(rotation=0,size='xx-large')
plt.title('有无客舱号死亡与存活人数比例')
plt.xlabel('')
plt.legend()
plt.show()

full['Title'] = full['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())
print(full.Title.value_counts())
print(pd.crosstab(full.Title,full.Sex))
full[(full.Title=='Dr')&(full.Sex=='female')]

nn={'Capt':'Rareman', 'Col':'Rareman','Don':'Rareman','Dona':'Rarewoman',
    'Dr':'Rareman','Jonkheer':'Rareman','Lady':'Rarewoman','Major':'Rareman',
    'Master':'Master','Miss':'Miss','Mlle':'Rarewoman','Mme':'Rarewoman',
    'Mr':'Mr','Mrs':'Mrs','Ms':'Rarewoman','Rev':'Mr','Sir':'Rareman',
    'the Countess':'Rarewoman'}
full.Title=full.Title.map(nn)
# 将 female 'Dr' 的Title映射为 'Rarewoman'
full.loc[full.PassengerId==797,'Title']='Rarewoman'
print(full.Title.value_counts())
full.Age.fillna(999, inplace=True)


def girl(aa):
    if (aa.Age != 999) & (aa.Title == 'Miss') & (aa.Age <= 14):
        return 'Girl'
    elif (aa.Age == 999) & (aa.Title == 'Miss') & (aa.Parch != 0):
        return 'Girl'
    else:
        return aa.Title


full['Title'] = full.apply(girl, axis=1)

Tit = ['Mr', 'Miss', 'Mrs', 'Master', 'Girl', 'Rareman', 'Rarewoman']
for i in Tit:
    full.loc[(full.Age == 999) & (full.Title == i), 'Age'] = full.loc[full.Title == i, 'Age'].median()
print(full.isnull().sum()) # 查看数据的缺失情况
print(full.info())
Sex=pd.crosstab(full.Sex,full.Survived)
Sex.rename(index={'female':'女性','male':'男性'},columns={0.0:'死亡',1.0:'存活'},inplace=True)
Sex.plot.bar(figsize=(8,5),color=['red','blue'])
plt.xticks(rotation=0,size='large')
plt.title('男女性别中死亡存活人数')
plt.legend()
plt.show()
Age=pd.concat([full[full.Survived==1]['Age'],full[full.Survived==0]['Age']],axis=1)
Age.columns.values[0]='存活'
Age.rename(columns={0:'存活','Age':'死亡'},inplace=True)
print(Age)

Age.plot(kind='hist',bins=30,figsize=(15,8),alpha=0.3,color=['red','green'])
plt.show()

Fare=pd.concat([full[full.Survived==1]['Fare'],full[full.Survived==0]['Fare']],axis=1)
Fare.columns.values[0]='存活'
Fare.rename(columns={0:'存活','Fare':'死亡'},inplace=True)
print(Fare)
Fare.plot(kind='hist',bins=30,figsize=(15,8),alpha=0.3,color=['red','green'])
plt.show()

full.groupby(['Title'])[['Title','Survived']].mean().plot(kind='bar',figsize=(10,7))
plt.xticks(rotation=0)
plt.show()
fig,axes=plt.subplots(2,3,figsize=(15,8))
Sex1=['male','female']
for i,ax in zip(Sex1,axes):
    for j,pp in zip(range(1,4),ax):
        PclassSex=full[(full.Sex==i)&(full.Pclass==j)]['Survived'].value_counts().sort_index(ascending=False)
        pp.bar(range(len(PclassSex)),PclassSex,label=(i,'Class'+str(j)))
        pp.set_xticks((0,1))
        pp.set_xticklabels(('Survived','Dead'))
        pp.legend(bbox_to_anchor=(0.6,1.1))
plt.show()
# 使用 pd.cut 将年龄平均分成5个区间
full.AgeCut=pd.cut(full.Age,5)
# 使用 pd.cut 将费用平均分成5个区间
full.FareCut=pd.qcut(full.Fare,5)
print(full.AgeCut.value_counts().sort_index())
print(full.FareCut.value_counts().sort_index())
# 根据各个分段 重新给 AgeCut赋值
full.loc[full.Age<=16.136,'AgeCut']=1
full.loc[(full.Age>16.136)&(full.Age<=32.102),'AgeCut']=2
full.loc[(full.Age>32.102)&(full.Age<=48.068),'AgeCut']=3
full.loc[(full.Age>48.068)&(full.Age<=64.034),'AgeCut']=4
full.loc[full.Age>64.034,'AgeCut']=5
# 根据各个分段 重新给 FareCut赋值
full.loc[full.Fare<=7.854,'FareCut']=1
full.loc[(full.Fare>7.854)&(full.Fare<=10.5),'FareCut']=2
full.loc[(full.Fare>10.5)&(full.Fare<=21.558),'FareCut']=3
full.loc[(full.Fare>21.558)&(full.Fare<=41.579),'FareCut']=4
full.loc[full.Fare>41.579,'FareCut']=5


full[['FareCut','Survived']].groupby(['FareCut']).mean().plot.bar(figsize=(8,5))
plt.show()
print(full.corr())
full[full.Survived.notnull()].pivot_table(index=['Title', 'Pclass'], values=['Survived']).sort_values('Survived',
                                                                                                      ascending=False)

full[full.Survived.notnull()].pivot_table(index=['Title', 'Parch'], values=['Survived']).sort_values('Survived',
                                                                                                     ascending=False)
# use 'Title','Pclass','Parch' to generate feature 'TPP'.
Tit=['Girl','Master','Mr','Miss','Mrs','Rareman','Rarewoman']
for i in Tit:
    for j in range(1,4):
        for g in range(0,10):
            if full.loc[(full.Title==i)&(full.Pclass==j)&(full.Parch==g)&(full.Survived.notnull()),'Survived'].mean()>=0.8:
                full.loc[(full.Title==i)&(full.Pclass==j)&(full.Parch==g),'TPP']=1
            elif full.loc[(full.Title==i)&(full.Pclass==j)&(full.Parch==g)&(full.Survived.notnull()),'Survived'].mean()>=0.5:
                full.loc[(full.Title==i)&(full.Pclass==j)&(full.Parch==g),'TPP']=2
            elif full.loc[(full.Title==i)&(full.Pclass==j)&(full.Parch==g)&(full.Survived.notnull()),'Survived'].mean()>=0:
                full.loc[(full.Title==i)&(full.Pclass==j)&(full.Parch==g),'TPP']=3
            else:
                full.loc[(full.Title==i)&(full.Pclass==j)&(full.Parch==g),'TPP']=4
print(full[full.TPP==4])
full.loc[(full.TPP==4)&(full.Sex=='female')&(full.Pclass!=3),'TPP']=1
full.loc[(full.TPP==4)&(full.Sex=='female')&(full.Pclass==3),'TPP']=2
full.loc[(full.TPP==4)&(full.Sex=='male')&(full.Pclass!=3),'TPP']=2
full.loc[(full.TPP==4)&(full.Sex=='male')&(full.Pclass==3),'TPP']=3
print(full.TPP.value_counts())
print(full.info())
predictors = ['Cabin', 'Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Title', 'AgeCut', 'TPP', 'FareCut', 'Age',
              'Fare']

# 使用 one-hot encoding 将分类变量转为数值变量
# pd.get_dummies使用方法 jjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj
full_dummies = pd.get_dummies(full[predictors])
print(full_dummies.head())
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
X=full_dummies[:891] # 训练集
y=full.Survived[:891] # 标签


test_X=full_dummies[891:] # 测试集
from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()
X_scaled=scaler.fit(X).transform(X) # scaler.fit(X)计算出 X 均值和方差然后再转换成标准的正态分布


test_X_scaled=scaler.fit(X).transform(test_X)
models=[KNeighborsClassifier(),LogisticRegression(),GaussianNB(),DecisionTreeClassifier(),RandomForestClassifier(),
GradientBoostingClassifier(),SVC()]
# evaluate models by using cross-validation

names = ['KNN', 'LR', 'NB', 'Tree', 'RF', 'GDBT', 'SVM']
for name, model in zip(names, models):
    score = cross_val_score(model, X, y, cv=5)
    print("{}:{},{}".format(name, score.mean(), score))
# 使用标准化的数据 scaled data
names=['KNN','LR','NB','Tree','RF','GDBT','SVM']
for name, model in zip(names,models):
    score=cross_val_score(model,X_scaled,y,cv=5)
    print("{}:{},{}".format(name,score.mean(),score))
model=GradientBoostingClassifier()
model.fit(X,y)
print(model.feature_importances_)
print(X.columns)
fi=pd.DataFrame({'importance':model.feature_importances_},index=X.columns)
print(fi.sort_values('importance',ascending=False))
fi.sort_values('importance',ascending=False)
fi.columns.values[0]='重要程度'
fi.rename(columns={0:'重要程度'},inplace=True)
fi.plot.bar(figsize=(11,7))


plt.xticks(rotation=30)
plt.title('重要特征',size='x-large')

plt.show()
from sklearn.model_selection import KFold

kf = KFold(n_splits=10, random_state=1,shuffle=True)

# extract the indices of misclassified observations
rr = []
for train_index, val_index in kf.split(X):
    pred = model.fit(X.loc[train_index], y[train_index]).predict(X.loc[val_index])
    rr.append(y[val_index][pred != y[val_index]].index.values)

# combine all the indices
whole_index = np.concatenate(rr)
len(whole_index)
print(full.loc[whole_index].head())
diff=full.loc[whole_index]
diff.describe()
diff.describe(include=['O'])
# both mean and count of 'survived' should be considered.
diff.groupby(['Title'])['Survived'].agg([('average','mean'),('number','count')])
diff.groupby(['Title','Pclass'])['Survived'].agg([('average','mean'),('number','count')])
diff.groupby(['Title','Pclass','Parch','SibSp'])['Survived'].agg([('average','mean'),('number','count')])
full.loc[(full.Title=='Mr')&(full.Pclass==1)&(full.Parch==0)&((full.SibSp==0)|(full.SibSp==1)),'MPPS']=1
full.loc[(full.Title=='Mr')&(full.Pclass!=1)&(full.Parch==0)&(full.SibSp==0),'MPPS']=2
full.loc[(full.Title=='Miss')&(full.Pclass==3)&(full.Parch==0)&(full.SibSp==0),'MPPS']=3
full.MPPS.fillna(4,inplace=True)
print(full.MPPS.value_counts())
diff[(diff.Title=='Mr')|(diff.Title=='Miss')].groupby(['Title','Survived','Pclass'])[['Fare']].describe().unstack()
full[(full.Title=='Mr')|(full.Title=='Miss')].groupby(['Title','Survived','Pclass'])[['Fare']].describe().unstack()
colormap = plt.cm.viridis
plt.figure(figsize=(12,12))
plt.title('热力图', y=1.05, size=20)
sns.heatmap(full[['Cabin','Parch','Pclass','SibSp','AgeCut','TPP','FareCut','Age','Fare','MPPS','Survived']].astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)
plt.show()
predictors=['Cabin','Embarked','Parch','Pclass','Sex','SibSp','Title','AgeCut','TPP','FareCut','Age','Fare','MPPS']
full_dummies=pd.get_dummies(full[predictors])
X=full_dummies[:891]
y=full.Survived[:891]
test_X=full_dummies[891:]

scaler=StandardScaler()
X_scaled=scaler.fit(X).transform(X)
test_X_scaled=scaler.fit(X).transform(test_X)
from sklearn.model_selection import GridSearchCV
param_grid={'n_neighbors':[1,2,3,4,5,6,7,8,9]}
grid_search=GridSearchCV(KNeighborsClassifier(),param_grid,cv=5)

grid_search.fit(X_scaled,y)

print(grid_search.best_params_,grid_search.best_score_)
param_grid={'C':[0.01,0.1,1,10]}
grid_search=GridSearchCV(LogisticRegression(),param_grid,cv=5)

grid_search.fit(X_scaled,y)

print(grid_search.best_params_,grid_search.best_score_)
# second round grid search
param_grid={'C':[0.04,0.06,0.08,0.1,0.12,0.14]}
grid_search=GridSearchCV(LogisticRegression(),param_grid,cv=5)

grid_search.fit(X_scaled,y)

print(grid_search.best_params_,grid_search.best_score_)
param_grid={'C':[0.01,0.1,1,10],'gamma':[0.01,0.1,1,10]}
grid_search=GridSearchCV(SVC(),param_grid,cv=5)

grid_search.fit(X_scaled,y)

print(grid_search.best_params_,grid_search.best_score_)
#second round grid search
param_grid={'C':[2,4,6,8,10,12,14],'gamma':[0.008,0.01,0.012,0.015,0.02]}
grid_search=GridSearchCV(SVC(),param_grid,cv=5)

grid_search.fit(X_scaled,y)

print(grid_search.best_params_,grid_search.best_score_)
param_grid={'n_estimators':[30,50,80,120,200],'learning_rate':[0.05,0.1,0.5,1],'max_depth':[1,2,3,4,5]}
grid_search=GridSearchCV(GradientBoostingClassifier(),param_grid,cv=5)

grid_search.fit(X_scaled,y)

print(grid_search.best_params_,grid_search.best_score_)
#second round search
param_grid={'n_estimators':[100,120,140,160],'learning_rate':[0.05,0.08,0.1,0.12],'max_depth':[3,4]}
grid_search=GridSearchCV(GradientBoostingClassifier(),param_grid,cv=5)

grid_search.fit(X_scaled,y)

print(grid_search.best_params_,grid_search.best_score_)
from sklearn.ensemble import BaggingClassifier
bagging=BaggingClassifier(LogisticRegression(C=0.14),n_estimators=100)
from sklearn.ensemble import VotingClassifier
clf1=LogisticRegression(C=0.14)
clf2=RandomForestClassifier(n_estimators=500)
clf3=GradientBoostingClassifier(n_estimators=120,learning_rate=0.12,max_depth=4)
clf4=SVC(C=6,gamma=0.01,probability=True)
clf5=KNeighborsClassifier(n_neighbors=8)
eclf_hard=VotingClassifier(estimators=[('LR',clf1),('RF',clf2),('GDBT',clf3),('SVM',clf4),('KNN',clf5)])
# add weights
eclfW_hard=VotingClassifier(estimators=[('LR',clf1),('RF',clf2),('GDBT',clf3),('SVM',clf4),('KNN',clf5)],weights=[1,1,2,2,1])
# soft voting
eclf_soft=VotingClassifier(estimators=[('LR',clf1),('RF',clf2),('GDBT',clf3),('SVM',clf4),('KNN',clf5)],voting='soft')
# add weights
eclfW_soft=VotingClassifier(estimators=[('LR',clf1),('RF',clf2),('GDBT',clf3),('SVM',clf4),('KNN',clf5)],voting='soft',weights=[1,1,2,2,1])
models=[KNeighborsClassifier(n_neighbors=8),LogisticRegression(C=0.14),GaussianNB(),DecisionTreeClassifier(),RandomForestClassifier(n_estimators=500),
        GradientBoostingClassifier(n_estimators=120,learning_rate=0.12,max_depth=4),SVC(C=6,gamma=0.01),
        eclf_hard,eclf_soft,eclfW_hard,eclfW_soft,bagging]
names=['KNN','LR','NB','CART','RF','GBT','SVM','VC_hard','VC_soft','VCW_hard','VCW_soft','Bagging']
for name,model in zip(names,models):
    score=cross_val_score(model,X_scaled,y,cv=5)
    print("{}: {},{}".format(name,score.mean(),score))
from sklearn.model_selection import StratifiedKFold

n_train = train.shape[0]
n_test = test.shape[0]
kf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)


def get_oof(clf, X, y, test_X):
    oof_train = np.zeros((n_train,))
    oof_test_mean = np.zeros((n_test,))
    oof_test_single = np.empty((5, n_test))
    for i, (train_index, val_index) in enumerate(kf.split(X, y)):
        kf_X_train = X[train_index]
        kf_y_train = y[train_index]
        kf_X_val = X[val_index]

        clf.fit(kf_X_train, kf_y_train)

        oof_train[val_index] = clf.predict(kf_X_val)
        oof_test_single[i, :] = clf.predict(test_X)
    oof_test_mean = oof_test_single.mean(axis=0)
    return oof_train.reshape(-1, 1), oof_test_mean.reshape(-1, 1)


LR_train, LR_test = get_oof(LogisticRegression(C=0.14), X_scaled, y, test_X_scaled)
KNN_train, KNN_test = get_oof(KNeighborsClassifier(n_neighbors=8), X_scaled, y, test_X_scaled)
SVM_train, SVM_test = get_oof(SVC(C=6, gamma=0.01), X_scaled, y, test_X_scaled)
GBDT_train, GBDT_test = get_oof(GradientBoostingClassifier(n_estimators=120, learning_rate=0.12, max_depth=4), X_scaled,
                                y, test_X_scaled)
X_stack = np.concatenate((LR_train, KNN_train, SVM_train, GBDT_train), axis=1)
y_stack = y
X_test_stack = np.concatenate((LR_test, KNN_test, SVM_test, GBDT_test), axis=1)
print(X_stack.shape, y_stack.shape, X_test_stack.shape)
stack_score=cross_val_score(RandomForestClassifier(n_estimators=1000),X_stack,y_stack,cv=5)
# cross-validation score of stacking
print(stack_score.mean(),stack_score)
#pred=RandomForestClassifier(n_estimators=1000).fit(X_stack,y_stack).predict(X_test_stack)
#pred=KNeighborsClassifier(n_neighbors=8).fit(X_stack,y_stack).predict(X_test_stack)
#pred=SVC(C=6, gamma=0.01).fit(X_stack,y_stack).predict(X_test_stack)
#pred=LogisticRegression(C=0.14).fit(X_stack,y_stack).predict(X_test_stack)

tt=pd.DataFrame({'PassengerId':test.PassengerId,'Survived':pred})
tt=tt.astype('int')


tt.to_csv(r'C:\Users\Administrator\Desktop\LogisticRegression.csv',index=False)
